{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leEoQoaqrH2N"
      },
      "source": [
        "# Sistema de anonimización para rostros en imagenes y videos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s80j6qWrmobV"
      },
      "source": [
        "## 1. Descripción del Problema\n",
        "En un contexto de creciente digitalización y vigilancia, proteger la identidad visual se ha vuelto crucial. Este proyecto busca resolver el problema de la exposición no consentida de rostros en medios digitales (imágenes y videos) mediante un sistema de detección y censura automática de rostros no autorizados.\n",
        "\n",
        "Relevancia legal:\n",
        "\n",
        "Ley 25.326 (Nacional): De protección de Datos Personales protege la imagen como dato sensible, exigiendo consentimiento para su uso o difusión.\n",
        "Ley 1845 (Ciudad Autónoma de Buenos Aires): Regula la protección de la imagen personal,                                                             prohibiendo la publicación o difusión de fotografías que identifiquen a una persona sin su autorización.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8UXf06anNeg"
      },
      "source": [
        "## 2. Diseño de la Solución\n",
        "La solución se basa en un sistema en cascada que detecta rostros, los reconoce si ya han sido autorizados (rostros conocidos) y aplica censura mediante desenfoque a los no reconocidos.\n",
        "Componentes clave:\n",
        "* InsightFace (RetinaFace): Detección precisa de rostros.\n",
        "* InceptionResnetV1 (FaceNet): Generación de vectores (embeddings) para comparar identidades.\n",
        "* OpenCV: Procesamiento de imagen/video y efectos de censura.\n",
        "* Gradio: Interfaz web interactiva para subir rostros conocidos y medios a procesar.\n",
        "\n",
        "Método de censura: desenfoque con elipse sobre la región facial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh_IxnrMnqm1"
      },
      "source": [
        "## 3. Código Implementado\n",
        "Dividido en secciones claras dentro del notebook:\n",
        "\n",
        "1. Instalación de librerías\n",
        "\n",
        "2. Importación de librerías y configuración\n",
        "\n",
        "3. Inicialización de modelos (detección + reconocimiento)\n",
        "\n",
        "4. Parámetros y estructuras para rostros conocidos\n",
        "\n",
        "5. Funciones auxiliares (embeddings, comparación)\n",
        "\n",
        "6. Censura facial con desenfoque\n",
        "\n",
        "7. Procesamiento de imagen/video\n",
        "\n",
        "8. Reconocimiento facial en tiempo real (frame por frame)\n",
        "\n",
        "9. Interfaz Gradio: carga rostros conocidos, procesamiento y descarga\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CREIOpqS5nor"
      },
      "outputs": [],
      "source": [
        "# Instalación de librerías necesarias para el sistema de detección y anonimizado facial\n",
        "\n",
        "!pip install -qq retina-face facenet-pytorch opencv-python scipy gradio insightface onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsBFEJoTKVt_",
        "outputId": "6e9942bf-5e93-41ad-e9ad-54d308b2c293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ],
      "source": [
        "# 1. Importación de librerías y configuración del entorno\n",
        "\n",
        "# Librerías del sistema y manipulación de archivos\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# Librerías para computación y procesamiento de imágenes\n",
        "import torch        # Computación en GPU, usado para modelos de deep learning\n",
        "import cv2          # OpenCV: procesamiento de imágenes y videos\n",
        "import numpy as np  # Operaciones numéricas con arreglos\n",
        "from PIL import Image  # Manejo de imágenes (carga, conversión, etc.)\n",
        "\n",
        "# Librerías para reconocimiento facial\n",
        "from facenet_pytorch import InceptionResnetV1  # Modelo de embeddings faciales\n",
        "from numpy.linalg import norm                  # Cálculo de norma para comparar vectores\n",
        "\n",
        "# Librerías para interfaz de usuario\n",
        "import gradio as gr  # Crear interfaz web para cargar imágenes y mostrar resultados\n",
        "\n",
        "# InsightFace para detección y análisis facial avanzado\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis  # Carga del sistema de análisis facial completo\n",
        "\n",
        "# Selección del dispositivo: usa GPU si está disponible, si no CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Usando dispositivo: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "c6c82275c54d4f68b4d8c248b6490e62",
            "231bb84daeae434188271b3eef10611d",
            "59d29d60f7934f5db0b867f6127efbfb",
            "cc336147c1c043faabf14767d4ed415f",
            "5dc0867491ad4f5bb494e25d55bc738f",
            "e48c263313d04fd4ad41fa1bb06e8ec3",
            "494bdc0aa7ea48edac3f6aafaa98b685",
            "27de5c9e941f4bc4a4e2483f11c02385",
            "949e104b5b7c4176936963d803c8eb6d",
            "24cd29a6301e466f8f7c3f2fe48b3479",
            "0a49b62e82a6450ebb83c79bfd8f4c50"
          ]
        },
        "id": "jtayZ7oyLVDF",
        "outputId": "5bbc55be-16c9-4739-998e-9d680d57db89"
      },
      "outputs": [],
      "source": [
        "# 2. Inicialización de modelos\n",
        "\n",
        "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == 'cuda' else ['CPUExecutionProvider']\n",
        "\n",
        "# Inicializa el detector facial de InsightFace (usa RetinaFace internamente)\n",
        "app = FaceAnalysis(allowed_modules=['detection'], providers=providers)\n",
        "app.prepare(ctx_id=0 if device == 'cuda' else -1, det_size=(640, 640))\n",
        "print(\"RetinaFace (via InsightFace) inicializado para detección.\")\n",
        "\n",
        "# Carga del modelo InceptionResnetV1 con pesos preentrenados (vggface2) para generar embeddings faciales\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "print(\"InceptionResnetV1 (para embeddings de rostros) inicializado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ztc7KUtZLihZ"
      },
      "outputs": [],
      "source": [
        "# 3. Parámetros generales y estructuras para rostros conocidos\n",
        "\n",
        "# Umbral de similitud para considerar que dos rostros son iguales\n",
        "THRESHOLD = 0.8\n",
        "\n",
        "# Crea carpeta para almacenar rostros conocidos (si no existe)\n",
        "os.makedirs(\"known_faces\", exist_ok=True)\n",
        "\n",
        "# Lista para guardar los vectores (embeddings) de rostros conocidos y sus nombres asociados a cada uno\n",
        "known_embeddings = []\n",
        "known_names = []\n",
        "\n",
        "USE_GRADIO = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o4tPe0SNLo-r"
      },
      "outputs": [],
      "source": [
        "# 4. Funciones auxiliares para embeddings y comparación de rostros\n",
        "\n",
        "# Convierte una imagen de rostro en un vector (embedding) usando InceptionResnetV1\n",
        "def get_embedding(face_img_pil):\n",
        "    if face_img_pil is None:\n",
        "        return None\n",
        "\n",
        "    # Redimensiona la imagen a 160x160 (tamaño requerido por el modelo)\n",
        "    face_img_pil = face_img_pil.resize((160, 160), Image.BILINEAR)\n",
        "\n",
        "    # Normaliza los valores de píxeles\n",
        "    face_np = np.array(face_img_pil).astype(np.float32)\n",
        "    face_np = (face_np - 127.5) / 128.0\n",
        "\n",
        "    # Convierte la imagen a tensor para PyTorch\n",
        "    face_tensor = torch.from_numpy(face_np).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    # Genera el embedding\n",
        "    with torch.no_grad():\n",
        "        emb = resnet(face_tensor.to(device))[0].cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "# Compara un embedding con los embeddings conocidos y devuelve True si coincide\n",
        "def is_known_face(embedding, known_embeddings, threshold=THRESHOLD):\n",
        "    if not known_embeddings:\n",
        "        return False\n",
        "    for known_emb in known_embeddings:\n",
        "        distance = norm(embedding - known_emb)\n",
        "        if distance < threshold:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "croWPowYLwoo"
      },
      "outputs": [],
      "source": [
        "# 5. Función de anonimización con máscara (blur)\n",
        "\n",
        "def censor_face_with_mask(image, box, landmarks, method='blur', ksize=25):\n",
        "    # Asegura que el tamaño del kernel sea impar (requisito de OpenCV)\n",
        "    ksize_blur = ksize if ksize % 2 == 1 else ksize + 1\n",
        "\n",
        "    # Define los límites del rostro y los ajusta al tamaño de la imagen\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    y1, y2 = max(0, y1), min(image.shape[0], y2)\n",
        "    x1, x2 = max(0, x1), min(image.shape[1], x2)\n",
        "\n",
        "    # Crea una máscara negra del mismo tamaño que la imagen\n",
        "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    # Calcula centros de ojos y boca a partir de los landmarks\n",
        "    eye_center_x = (landmarks[0][0] + landmarks[1][0]) / 2\n",
        "    eye_center_y = (landmarks[0][1] + landmarks[1][1]) / 2\n",
        "    mouth_center_x = (landmarks[3][0] + landmarks[4][0]) / 2\n",
        "    mouth_center_y = (landmarks[3][1] + landmarks[4][1]) / 2\n",
        "\n",
        "    # Calcula el centro de la elipse basada en ojos, nariz y boca\n",
        "    ellipse_center_x = int((eye_center_x + landmarks[2][0] + mouth_center_x) / 3)\n",
        "    ellipse_center_y = int((eye_center_y + landmarks[2][1] + mouth_center_y) / 3)\n",
        "\n",
        "    # Calcula ancho y alto de la elipse basada en distancias faciales\n",
        "    width = int(np.sqrt((landmarks[1][0] - landmarks[0][0])**2 + (landmarks[1][1] - landmarks[0][1])**2) * 2.5)\n",
        "    width = max(width, x2 - x1)\n",
        "\n",
        "    height = int(np.sqrt((mouth_center_x - eye_center_x)**2 + (mouth_center_y - eye_center_y)**2) * 2.8)\n",
        "    height = max(height, y2 - y1)\n",
        "\n",
        "    # Calcula el ángulo de inclinación de los ojos para rotar la elipse\n",
        "    delta_x_eyes = landmarks[1][0] - landmarks[0][0]\n",
        "    delta_y_eyes = landmarks[1][1] - landmarks[0][1]\n",
        "    angle = np.degrees(np.arctan2(delta_y_eyes, delta_x_eyes))\n",
        "\n",
        "    # Dibuja una elipse blanca en la máscara, sobre la zona del rostro\n",
        "    cv2.ellipse(mask, (ellipse_center_x, ellipse_center_y), (width // 2, height // 2),\n",
        "                angle, 0, 360, 255, -1)\n",
        "\n",
        "    # Suaviza y expande ligeramente la máscara para mejorar el resultado\n",
        "    mask = cv2.dilate(mask, None, iterations=2)\n",
        "    mask = cv2.GaussianBlur(mask, (ksize_blur, ksize_blur), 0)\n",
        "\n",
        "    # Normaliza la máscara a rango [0,1] y la expande a 3 canales\n",
        "    mask_normalized = mask.astype(np.float32) / 255.0\n",
        "    mask_normalized = np.stack([mask_normalized, mask_normalized, mask_normalized], axis=-1)\n",
        "\n",
        "    # Crea una copia de la imagen para aplicar censura\n",
        "    temp_image = image.copy()\n",
        "\n",
        "    # Aplica el método de censura seleccionado\n",
        "    if method == 'blur':\n",
        "        censored_full_image = cv2.GaussianBlur(temp_image, (ksize_blur, ksize_blur), 0)\n",
        "    elif method == 'pixelate':\n",
        "        h_full, w_full = temp_image.shape[:2]\n",
        "        ksize_pixelate = max(1, ksize_blur // 5)\n",
        "        temp = cv2.resize(temp_image, (max(1, w_full // ksize_pixelate), max(1, h_full // ksize_pixelate)), interpolation=cv2.INTER_LINEAR)\n",
        "        censored_full_image = cv2.resize(temp, (w_full, h_full), interpolation=cv2.INTER_NEAREST)\n",
        "    else:\n",
        "        censored_full_image = temp_image\n",
        "\n",
        "    # Combina imagen original y censurada usando la máscara como alpha\n",
        "    result_image = (censored_full_image * mask_normalized) + (image * (1 - mask_normalized))\n",
        "    result_image = np.uint8(result_image)\n",
        "\n",
        "    return result_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8NPA4UYeMCDC"
      },
      "outputs": [],
      "source": [
        "# 6. Reconocimiento y anonimización de rostros en un frame\n",
        "\n",
        "def recognize_and_censor(frame, known_embeddings, censor_method='blur', blur_ksize=35):\n",
        "    # Convierte la imagen de BGR (OpenCV) a RGB (requerido por InsightFace)\n",
        "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detecta rostros en la imagen usando InsightFace\n",
        "    faces = app.get(img_rgb)\n",
        "\n",
        "    # Si no se detectan rostros, retorna la imagen original\n",
        "    if not faces:\n",
        "        return frame\n",
        "\n",
        "    # Procesa cada rostro detectado\n",
        "    for face in faces:\n",
        "        box = face.bbox          # Caja delimitadora del rostro\n",
        "        landmarks = face.kps     # Puntos clave (ojos, nariz, boca)\n",
        "\n",
        "        # Ajusta los límites de la caja al tamaño de la imagen\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
        "\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "\n",
        "        # Extrae el rostro como un recorte del frame\n",
        "        face_img_cropped_np = frame[y1:y2, x1:x2]\n",
        "        if face_img_cropped_np.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Convierte el recorte a formato PIL y RGB para el modelo de embeddings\n",
        "        face_img_pil = Image.fromarray(cv2.cvtColor(face_img_cropped_np, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Obtiene el embedding del rostro\n",
        "        emb = get_embedding(face_img_pil)\n",
        "\n",
        "        # Si no es un rostro conocido, lo blurea\n",
        "        if emb is not None and not is_known_face(emb, known_embeddings):\n",
        "            frame = censor_face_with_mask(frame, box, landmarks, method=censor_method, ksize=blur_ksize)\n",
        "\n",
        "    # Devuelve el frame con rostros anonimizados (si corresponde)\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3AXMyK_jMLe0"
      },
      "outputs": [],
      "source": [
        "# 7. Detección si un archivo es video y conversión de .webm a .mp4\n",
        "\n",
        "def is_video(filename):\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.webm']\n",
        "    return any(filename.lower().endswith(ext) for ext in video_extensions)\n",
        "\n",
        "def convert_webm_to_mp4(input_path, output_path='temp_converted.mp4'):\n",
        "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
        "    cmd = [\n",
        "        'ffmpeg', '-y',\n",
        "        '-i', input_path,\n",
        "        '-vf', 'scale=trunc(iw/2)*2:trunc(ih/2)*2',\n",
        "        '-c:v', 'libx264',\n",
        "        '-pix_fmt', 'yuv420p',\n",
        "        output_path\n",
        "    ]\n",
        "    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if result.returncode != 0:\n",
        "        raise RuntimeError(f\"La conversión con ffmpeg falló: {result.stderr}\")\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d-X66v12MT4r"
      },
      "outputs": [],
      "source": [
        "# 8. Procesamiento general de un archivo (imagen o video)\n",
        "\n",
        "def process_single_file(file_path, censor_method='blur', blur_ksize=35):\n",
        "    global known_embeddings\n",
        "\n",
        "    output_dir = \"processed_media\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    base_name = os.path.basename(file_path)\n",
        "    name, ext = os.path.splitext(base_name)\n",
        "\n",
        "    file_path_to_process = file_path\n",
        "    temp_mp4_path = None\n",
        "\n",
        "    if ext.lower() == '.webm':\n",
        "        temp_mp4_path = os.path.join(output_dir, f\"temp_{name}.mp4\")\n",
        "        file_path_to_process = convert_webm_to_mp4(file_path, temp_mp4_path)\n",
        "\n",
        "    # Procesamiento si es video\n",
        "    if is_video(file_path_to_process):\n",
        "        cap = cv2.VideoCapture(file_path_to_process)\n",
        "        if not cap.isOpened():\n",
        "            if temp_mp4_path and os.path.exists(temp_mp4_path):\n",
        "                os.remove(temp_mp4_path)\n",
        "            raise ValueError(f\"No se pudo abrir el video: {file_path_to_process}\")\n",
        "\n",
        "        # Configuración del archivo de salida\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        output_path = os.path.join(output_dir, f\"censored_video_{name}.mp4\")\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "        if not out.isOpened():\n",
        "            cap.release()\n",
        "            if temp_mp4_path and os.path.exists(temp_mp4_path):\n",
        "                os.remove(temp_mp4_path)\n",
        "            raise ValueError(f\"No se pudo crear el archivo de salida de video: {output_path}. Asegúrate de que los códecs estén disponibles.\")\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = recognize_and_censor(frame, known_embeddings, censor_method, blur_ksize)\n",
        "            out.write(frame)\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Elimina archivo temporal si fue creado\n",
        "        if temp_mp4_path and os.path.exists(temp_mp4_path):\n",
        "            os.remove(temp_mp4_path)\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    # Procesamiento si es imagen\n",
        "    else:\n",
        "        image = cv2.imread(file_path_to_process)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"No se pudo abrir la imagen: {file_path_to_process}\")\n",
        "\n",
        "        # Aplica censura a la imagen\n",
        "        image = recognize_and_censor(image, known_embeddings, censor_method, blur_ksize)\n",
        "        output_path = os.path.join(output_dir, f\"censored_image_{name}.jpg\")\n",
        "        cv2.imwrite(output_path, image)\n",
        "        return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "9mmtLcd2-jyf",
        "outputId": "fd20ef19-a213-4694-8aa0-fe6b1550bc1e"
      },
      "outputs": [],
      "source": [
        "# 9. Gradio\n",
        "\n",
        "if USE_GRADIO:\n",
        "\n",
        "    # Carga de rostros conocidos\n",
        "    def load_known_faces_gradio(face_files):\n",
        "        global known_embeddings, known_names\n",
        "        known_embeddings = []\n",
        "        known_names = []\n",
        "\n",
        "        if face_files:\n",
        "            for face_file in face_files:\n",
        "                img_path = face_file.name\n",
        "                img_pil = Image.open(img_path).convert(\"RGB\")\n",
        "                name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "                img_np_rgb = np.array(img_pil)\n",
        "                faces_in_known_img = app.get(img_np_rgb)\n",
        "\n",
        "                if len(faces_in_known_img) == 0:\n",
        "                    print(f\"Advertencia: No se detectó ningún rostro en '{name}'. Saltando este archivo.\")\n",
        "                    continue\n",
        "                elif len(faces_in_known_img) > 1:\n",
        "                    print(f\"Advertencia: Se detectaron múltiples rostros en '{name}'. Se usará el primer rostro detectado para el embedding.\")\n",
        "\n",
        "                face = faces_in_known_img[0]\n",
        "                box = face.bbox\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "                x1, y1 = max(0, x1), max(0, y1)\n",
        "                x2, y2 = min(img_np_rgb.shape[1], x2), min(img_np_rgb.shape[0], y2)\n",
        "\n",
        "                cropped_face_np = img_np_rgb[y1:y2, x1:x2]\n",
        "                if cropped_face_np.size == 0:\n",
        "                    print(f\"Advertencia: El recorte del rostro en '{name}' está vacío. Saltando.\")\n",
        "                    continue\n",
        "\n",
        "                cropped_face_pil = Image.fromarray(cropped_face_np)\n",
        "\n",
        "                emb = get_embedding(cropped_face_pil)\n",
        "\n",
        "                if emb is not None:\n",
        "                    known_embeddings.append(emb)\n",
        "                    known_names.append(name)\n",
        "                else:\n",
        "                    print(f\"Advertencia: No se pudo obtener la incrustación para el rostro en '{name}'. Saltando.\")\n",
        "            return f\"{len(known_embeddings)} rostros conocidos cargados.\"\n",
        "        return \"No se cargaron rostros conocidos.\"\n",
        "\n",
        "    # Función que procesa múltiples archivos, los guarda y genera un ZIP\n",
        "    def process_and_zip_and_display(files_to_process, censor_method, blur_ksize):\n",
        "        output_dir = \"processed_media\"\n",
        "        if os.path.exists(output_dir):\n",
        "            shutil.rmtree(output_dir)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        output_dir_for_zip = \"processed_media_for_zip_temp\"\n",
        "        if os.path.exists(output_dir_for_zip):\n",
        "            shutil.rmtree(output_dir_for_zip)\n",
        "        os.makedirs(output_dir_for_zip, exist_ok=True)\n",
        "\n",
        "        if not files_to_process:\n",
        "            return (\n",
        "                \"Por favor, sube al menos una imagen o video para procesar.\",\n",
        "                None,\n",
        "                gr.update(visible=False)\n",
        "            )\n",
        "\n",
        "        processed_images_paths = []\n",
        "        status_messages = []\n",
        "\n",
        "        for uploaded_file in files_to_process:\n",
        "            try:\n",
        "                output_path = process_single_file(uploaded_file.name, censor_method, blur_ksize)\n",
        "                if not is_video(output_path):\n",
        "                    processed_images_paths.append(output_path)\n",
        "                status_messages.append(f\"{os.path.basename(uploaded_file.name)} procesado correctamente. Salida: {output_path}\")\n",
        "            except Exception as e:\n",
        "                status_messages.append(f\"Error procesando {os.path.basename(uploaded_file.name)}: {str(e)}\")\n",
        "\n",
        "        # Genera ZIP con todos los archivos procesados si hay alguno\n",
        "        zip_path = None\n",
        "        if os.listdir(output_dir):\n",
        "            zip_base_name = os.path.join(output_dir_for_zip, \"processed_media_output\")\n",
        "            shutil.make_archive(zip_base_name, 'zip', output_dir)\n",
        "            zip_path = zip_base_name + \".zip\"\n",
        "\n",
        "        return (\n",
        "            \"\\n\".join(status_messages),\n",
        "            processed_images_paths if processed_images_paths else None,\n",
        "            gr.File(label=\"Descargar todos los medios procesados (ZIP)\", value=zip_path, visible=True if zip_path else False)\n",
        "        )\n",
        "\n",
        "    # Interfaz Gradio\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# Anonimización de Rostros con Detección Avanzada\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### 1. Cargar Rostros Conocidos\")\n",
        "        gr.Markdown(\"Sube imágenes de los rostros que no quieres que sean anonimizados.\")\n",
        "        known_faces_input = gr.File(\n",
        "            label=\"Archivos de Rostros Conocidos (JPEG/PNG)\",\n",
        "            file_count=\"multiple\",\n",
        "            type=\"filepath\",\n",
        "            file_types=[\"image\"]\n",
        "        )\n",
        "        load_button = gr.Button(\"Cargar Rostros Conocidos\")\n",
        "        load_output = gr.Textbox(label=\"Estado de Carga de Rostros\")\n",
        "        load_button.click(load_known_faces_gradio, inputs=known_faces_input, outputs=load_output)\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### 2. Procesar Medios\")\n",
        "        gr.Markdown(\"Sube las imágenes o videos que deseas procesar para anonimizar rostros.\")\n",
        "        file_input = gr.File(\n",
        "            label=\"Imágenes o Videos (JPEG/PNG/MP4/MOV/WEBM)\",\n",
        "            file_count=\"multiple\",\n",
        "            type=\"filepath\"\n",
        "        )\n",
        "        censor_method_radio = gr.Radio(\n",
        "            [\"blur\"],\n",
        "            label=\"Método de anonimizado\",\n",
        "            value=\"blur\"\n",
        "        )\n",
        "        blur_ksize_slider = gr.Slider(\n",
        "            minimum=5,\n",
        "            maximum=101,\n",
        "            step=2,\n",
        "            value=35,\n",
        "            label=\"Tamaño de blureo (Cuanto mayor, más intenso el efecto)\"\n",
        "        )\n",
        "        process_button = gr.Button(\"Procesar Medios\")\n",
        "\n",
        "        processing_status = gr.Textbox(label=\"Estado de Procesamiento\", lines=5)\n",
        "        output_image_gallery = gr.Gallery(\n",
        "            label=\"Imágenes Procesadas (Solo imágenes mostradas aquí, videos en el ZIP)\",\n",
        "            columns=4,\n",
        "            height=\"auto\",\n",
        "            object_fit=\"contain\",\n",
        "            visible=True\n",
        "        )\n",
        "        output_zip_file = gr.File(\n",
        "            label=\"Descargar todos los medios procesados (ZIP)\",\n",
        "            visible=False\n",
        "        )\n",
        "\n",
        "        process_button.click(\n",
        "            process_and_zip_and_display,\n",
        "            inputs=[file_input, censor_method_radio, blur_ksize_slider],\n",
        "            outputs=[processing_status, output_image_gallery, output_zip_file]\n",
        "        )\n",
        "\n",
        "    demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edI1Jb6NqORA"
      },
      "source": [
        "## 4. Resultados Obtenidos\n",
        "* Detección precisa: Usando RetinaFace se logra detectar con precisión aún con múltiples rostros en escena.\n",
        "\n",
        "* Censura efectiva: Aplicación de desenfoque sobre áreas sensibles (ojos, nariz, boca).\n",
        "\n",
        "* Reconocimiento: Capacidad de no censurar rostros previamente autorizados.\n",
        "\n",
        "* Interfaz usable: Gradio permite a cualquier usuario interactuar fácilmente sin necesidad de código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVzz1Wfovt_"
      },
      "source": [
        "## 5. Conclusiones y Posibles Mejoras\n",
        "Conclusiones\n",
        "\n",
        "* El sistema cumple exitosamente con su objetivo de proteger la identidad visual de las personas en imágenes y videos mediante censura automática de rostros no autorizados.\n",
        "\n",
        "* Su uso es especialmente relevante en contextos como medios de comunicación, redes sociales, monitoreo urbano o seguridad institucional.\n",
        "\n",
        "* La incorporación de una interfaz amigable permite que cualquier usuario, sin conocimientos técnicos, pueda aplicar estas medidas de protección.\n",
        "\n",
        "Posibles mejoras\n",
        "* Incrementar la precisión del reconocimiento permitiendo asociar múltiples imágenes por persona para mejorar la robustez del modelo ante variaciones de pose, iluminación o expresión facial.\n",
        "\n",
        "* Implementar almacenamiento persistente de rostros conocidos, para que una vez cargados no sea necesario volver a subirlos en cada sesión del sistema.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a49b62e82a6450ebb83c79bfd8f4c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "231bb84daeae434188271b3eef10611d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48c263313d04fd4ad41fa1bb06e8ec3",
            "placeholder": "​",
            "style": "IPY_MODEL_494bdc0aa7ea48edac3f6aafaa98b685",
            "value": "100%"
          }
        },
        "24cd29a6301e466f8f7c3f2fe48b3479": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27de5c9e941f4bc4a4e2483f11c02385": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494bdc0aa7ea48edac3f6aafaa98b685": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59d29d60f7934f5db0b867f6127efbfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27de5c9e941f4bc4a4e2483f11c02385",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_949e104b5b7c4176936963d803c8eb6d",
            "value": 111898327
          }
        },
        "5dc0867491ad4f5bb494e25d55bc738f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "949e104b5b7c4176936963d803c8eb6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6c82275c54d4f68b4d8c248b6490e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_231bb84daeae434188271b3eef10611d",
              "IPY_MODEL_59d29d60f7934f5db0b867f6127efbfb",
              "IPY_MODEL_cc336147c1c043faabf14767d4ed415f"
            ],
            "layout": "IPY_MODEL_5dc0867491ad4f5bb494e25d55bc738f"
          }
        },
        "cc336147c1c043faabf14767d4ed415f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24cd29a6301e466f8f7c3f2fe48b3479",
            "placeholder": "​",
            "style": "IPY_MODEL_0a49b62e82a6450ebb83c79bfd8f4c50",
            "value": " 107M/107M [00:00&lt;00:00, 232MB/s]"
          }
        },
        "e48c263313d04fd4ad41fa1bb06e8ec3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
