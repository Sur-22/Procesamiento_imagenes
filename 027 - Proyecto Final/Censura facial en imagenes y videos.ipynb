{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c567e8bb50fe4ab2a629ffdc1794857f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a59567e571aa43809b3cc5fc9698897f",
              "IPY_MODEL_07521f10b2ba4aa5b0fe7ee92c8bbe0d",
              "IPY_MODEL_6f083829456e45259a2da7c3f840347b"
            ],
            "layout": "IPY_MODEL_b4c2ad2a3cde42f88617c1407c7921c1"
          }
        },
        "a59567e571aa43809b3cc5fc9698897f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdd74a95c3844f67a927fff343a2a5cf",
            "placeholder": "​",
            "style": "IPY_MODEL_58a3611674e84e6a83ac5a674e26a528",
            "value": "100%"
          }
        },
        "07521f10b2ba4aa5b0fe7ee92c8bbe0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0550ad0e5d8c416ab1fc64d4699558c8",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c954161383d426cb47adc60ec58822b",
            "value": 111898327
          }
        },
        "6f083829456e45259a2da7c3f840347b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98106dae04d740f5ba1044e2ced1f7f4",
            "placeholder": "​",
            "style": "IPY_MODEL_f14f0f303fd5455db363f13446490acf",
            "value": " 107M/107M [00:00&lt;00:00, 360MB/s]"
          }
        },
        "b4c2ad2a3cde42f88617c1407c7921c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd74a95c3844f67a927fff343a2a5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a3611674e84e6a83ac5a674e26a528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0550ad0e5d8c416ab1fc64d4699558c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c954161383d426cb47adc60ec58822b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98106dae04d740f5ba1044e2ced1f7f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f14f0f303fd5455db363f13446490acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Instalación de librerías necesarias para el sistema de detección y censura facial\n",
        "\n",
        "!pip install -qq retina-face facenet-pytorch opencv-python scipy gradio insightface onnxruntime"
      ],
      "metadata": {
        "id": "CREIOpqS5nor"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Importación de librerías y configuración del entorno\n",
        "\n",
        "# Librerías del sistema y manipulación de archivos\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# Librerías para computación y procesamiento de imágenes\n",
        "import torch        # Computación en GPU, usado para modelos de deep learning\n",
        "import cv2          # OpenCV: procesamiento de imágenes y videos\n",
        "import numpy as np  # Operaciones numéricas con arreglos\n",
        "from PIL import Image  # Manejo de imágenes (carga, conversión, etc.)\n",
        "\n",
        "# Librerías para reconocimiento facial\n",
        "from facenet_pytorch import InceptionResnetV1  # Modelo de embeddings faciales\n",
        "from numpy.linalg import norm                  # Cálculo de norma para comparar vectores\n",
        "\n",
        "# Librerías para interfaz de usuario\n",
        "import gradio as gr  # Crear interfaz web para cargar imágenes y mostrar resultados\n",
        "\n",
        "# InsightFace para detección y análisis facial avanzado\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis  # Carga del sistema de análisis facial completo\n",
        "\n",
        "# Selección del dispositivo: usa GPU si está disponible, si no CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Usando dispositivo: {device}\")"
      ],
      "metadata": {
        "id": "wsBFEJoTKVt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37dccb73-4d0b-45d4-dcc3-c4be5a25a2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Inicialización de modelos\n",
        "\n",
        "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device == 'cuda' else ['CPUExecutionProvider']\n",
        "\n",
        "# Inicializa el detector facial de InsightFace (usa RetinaFace internamente)\n",
        "app = FaceAnalysis(allowed_modules=['detection'], providers=providers)\n",
        "app.prepare(ctx_id=0 if device == 'cuda' else -1, det_size=(640, 640))\n",
        "print(\"RetinaFace (via InsightFace) inicializado para detección.\")\n",
        "\n",
        "# Carga del modelo InceptionResnetV1 con pesos preentrenados (vggface2) para generar embeddings faciales\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "print(\"InceptionResnetV1 (para embeddings de rostros) inicializado.\")\n"
      ],
      "metadata": {
        "id": "jtayZ7oyLVDF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "c567e8bb50fe4ab2a629ffdc1794857f",
            "a59567e571aa43809b3cc5fc9698897f",
            "07521f10b2ba4aa5b0fe7ee92c8bbe0d",
            "6f083829456e45259a2da7c3f840347b",
            "b4c2ad2a3cde42f88617c1407c7921c1",
            "cdd74a95c3844f67a927fff343a2a5cf",
            "58a3611674e84e6a83ac5a674e26a528",
            "0550ad0e5d8c416ab1fc64d4699558c8",
            "5c954161383d426cb47adc60ec58822b",
            "98106dae04d740f5ba1044e2ced1f7f4",
            "f14f0f303fd5455db363f13446490acf"
          ]
        },
        "outputId": "d3aa6576-135c-4099-a2c1-8a9e80215334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download_path: /root/.insightface/models/buffalo_l\n",
            "Downloading /root/.insightface/models/buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 281857/281857 [00:04<00:00, 65856.23KB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "model ignore: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "model ignore: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "model ignore: /root/.insightface/models/buffalo_l/genderage.onnx genderage\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "model ignore: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition\n",
            "set det-size: (640, 640)\n",
            "RetinaFace (via InsightFace) inicializado para detección.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c567e8bb50fe4ab2a629ffdc1794857f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InceptionResnetV1 (para embeddings de rostros) inicializado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Parámetros generales y estructuras para rostros conocidos\n",
        "\n",
        "# Umbral de similitud para considerar que dos rostros son iguales\n",
        "THRESHOLD = 0.8\n",
        "\n",
        "# Crea carpeta para almacenar rostros conocidos (si no existe)\n",
        "os.makedirs(\"known_faces\", exist_ok=True)\n",
        "\n",
        "# Lista para guardar los vectores (embeddings) de rostros conocidos y sus nombres asociados a cada uno\n",
        "known_embeddings = []\n",
        "known_names = []\n",
        "\n",
        "USE_GRADIO = True"
      ],
      "metadata": {
        "id": "Ztc7KUtZLihZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Funciones auxiliares para embeddings y comparación de rostros\n",
        "\n",
        "# Convierte una imagen de rostro en un vector (embedding) usando InceptionResnetV1\n",
        "def get_embedding(face_img_pil):\n",
        "    if face_img_pil is None:\n",
        "        return None\n",
        "\n",
        "    # Redimensiona la imagen a 160x160 (tamaño requerido por el modelo)\n",
        "    face_img_pil = face_img_pil.resize((160, 160), Image.BILINEAR)\n",
        "\n",
        "    # Normaliza los valores de píxeles\n",
        "    face_np = np.array(face_img_pil).astype(np.float32)\n",
        "    face_np = (face_np - 127.5) / 128.0\n",
        "\n",
        "    # Convierte la imagen a tensor para PyTorch\n",
        "    face_tensor = torch.from_numpy(face_np).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    # Genera el embedding\n",
        "    with torch.no_grad():\n",
        "        emb = resnet(face_tensor.to(device))[0].cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "# Compara un embedding con los embeddings conocidos y devuelve True si coincide\n",
        "def is_known_face(embedding, known_embeddings, threshold=THRESHOLD):\n",
        "    if not known_embeddings:\n",
        "        return False\n",
        "    for known_emb in known_embeddings:\n",
        "        distance = norm(embedding - known_emb)\n",
        "        if distance < threshold:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "o4tPe0SNLo-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Función de censura con máscara (blur o pixelate)\n",
        "\n",
        "def censor_face_with_mask(image, box, landmarks, method='blur', ksize=25):\n",
        "    # Asegura que el tamaño del kernel sea impar (requisito de OpenCV)\n",
        "    ksize_blur = ksize if ksize % 2 == 1 else ksize + 1\n",
        "\n",
        "    # Define los límites del rostro y los ajusta al tamaño de la imagen\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    y1, y2 = max(0, y1), min(image.shape[0], y2)\n",
        "    x1, x2 = max(0, x1), min(image.shape[1], x2)\n",
        "\n",
        "    # Crea una máscara negra del mismo tamaño que la imagen\n",
        "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    # Calcula centros de ojos y boca a partir de los landmarks\n",
        "    eye_center_x = (landmarks[0][0] + landmarks[1][0]) / 2\n",
        "    eye_center_y = (landmarks[0][1] + landmarks[1][1]) / 2\n",
        "    mouth_center_x = (landmarks[3][0] + landmarks[4][0]) / 2\n",
        "    mouth_center_y = (landmarks[3][1] + landmarks[4][1]) / 2\n",
        "\n",
        "    # Calcula el centro de la elipse basada en ojos, nariz y boca\n",
        "    ellipse_center_x = int((eye_center_x + landmarks[2][0] + mouth_center_x) / 3)\n",
        "    ellipse_center_y = int((eye_center_y + landmarks[2][1] + mouth_center_y) / 3)\n",
        "\n",
        "    # Calcula ancho y alto de la elipse basada en distancias faciales\n",
        "    width = int(np.sqrt((landmarks[1][0] - landmarks[0][0])**2 + (landmarks[1][1] - landmarks[0][1])**2) * 2.5)\n",
        "    width = max(width, x2 - x1)\n",
        "\n",
        "    height = int(np.sqrt((mouth_center_x - eye_center_x)**2 + (mouth_center_y - eye_center_y)**2) * 2.8)\n",
        "    height = max(height, y2 - y1)\n",
        "\n",
        "    # Calcula el ángulo de inclinación de los ojos para rotar la elipse\n",
        "    delta_x_eyes = landmarks[1][0] - landmarks[0][0]\n",
        "    delta_y_eyes = landmarks[1][1] - landmarks[0][1]\n",
        "    angle = np.degrees(np.arctan2(delta_y_eyes, delta_x_eyes))\n",
        "\n",
        "    # Dibuja una elipse blanca en la máscara, sobre la zona del rostro\n",
        "    cv2.ellipse(mask, (ellipse_center_x, ellipse_center_y), (width // 2, height // 2),\n",
        "                angle, 0, 360, 255, -1)\n",
        "\n",
        "    # Suaviza y expande ligeramente la máscara para mejorar el resultado\n",
        "    mask = cv2.dilate(mask, None, iterations=2)\n",
        "    mask = cv2.GaussianBlur(mask, (ksize_blur, ksize_blur), 0)\n",
        "\n",
        "    # Normaliza la máscara a rango [0,1] y la expande a 3 canales\n",
        "    mask_normalized = mask.astype(np.float32) / 255.0\n",
        "    mask_normalized = np.stack([mask_normalized, mask_normalized, mask_normalized], axis=-1)\n",
        "\n",
        "    # Crea una copia de la imagen para aplicar censura\n",
        "    temp_image = image.copy()\n",
        "\n",
        "    # Aplica el método de censura seleccionado\n",
        "    if method == 'blur':\n",
        "        censored_full_image = cv2.GaussianBlur(temp_image, (ksize_blur, ksize_blur), 0)\n",
        "    elif method == 'pixelate':\n",
        "        h_full, w_full = temp_image.shape[:2]\n",
        "        ksize_pixelate = max(1, ksize_blur // 5)\n",
        "        temp = cv2.resize(temp_image, (max(1, w_full // ksize_pixelate), max(1, h_full // ksize_pixelate)), interpolation=cv2.INTER_LINEAR)\n",
        "        censored_full_image = cv2.resize(temp, (w_full, h_full), interpolation=cv2.INTER_NEAREST)\n",
        "    else:\n",
        "        censored_full_image = temp_image\n",
        "\n",
        "    # Combina imagen original y censurada usando la máscara como alpha\n",
        "    result_image = (censored_full_image * mask_normalized) + (image * (1 - mask_normalized))\n",
        "    result_image = np.uint8(result_image)\n",
        "\n",
        "    return result_image"
      ],
      "metadata": {
        "id": "croWPowYLwoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Reconocimiento y censura de rostros en un frame\n",
        "\n",
        "def recognize_and_censor(frame, known_embeddings, censor_method='blur', blur_ksize=35):\n",
        "    # Convierte la imagen de BGR (OpenCV) a RGB (requerido por InsightFace)\n",
        "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detecta rostros en la imagen usando InsightFace\n",
        "    faces = app.get(img_rgb)\n",
        "\n",
        "    # Si no se detectan rostros, retorna la imagen original\n",
        "    if not faces:\n",
        "        return frame\n",
        "\n",
        "    # Procesa cada rostro detectado\n",
        "    for face in faces:\n",
        "        box = face.bbox          # Caja delimitadora del rostro\n",
        "        landmarks = face.kps     # Puntos clave (ojos, nariz, boca)\n",
        "\n",
        "        # Ajusta los límites de la caja al tamaño de la imagen\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
        "\n",
        "        if x2 <= x1 or y2 <= y1:\n",
        "            continue\n",
        "\n",
        "        # Extrae el rostro como un recorte del frame\n",
        "        face_img_cropped_np = frame[y1:y2, x1:x2]\n",
        "        if face_img_cropped_np.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Convierte el recorte a formato PIL y RGB para el modelo de embeddings\n",
        "        face_img_pil = Image.fromarray(cv2.cvtColor(face_img_cropped_np, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Obtiene el embedding del rostro\n",
        "        emb = get_embedding(face_img_pil)\n",
        "\n",
        "        # Si no es un rostro conocido, lo censura\n",
        "        if emb is not None and not is_known_face(emb, known_embeddings):\n",
        "            frame = censor_face_with_mask(frame, box, landmarks, method=censor_method, ksize=blur_ksize)\n",
        "\n",
        "    # Devuelve el frame con rostros censurados (si corresponde)\n",
        "    return frame"
      ],
      "metadata": {
        "id": "8NPA4UYeMCDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Detección si un archivo es video y conversión de .webm a .mp4\n",
        "\n",
        "def is_video(filename):\n",
        "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.webm']\n",
        "    return any(filename.lower().endswith(ext) for ext in video_extensions)\n",
        "\n",
        "def convert_webm_to_mp4(input_path, output_path='temp_converted.mp4'):\n",
        "    os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)\n",
        "    cmd = [\n",
        "        'ffmpeg', '-y',\n",
        "        '-i', input_path,\n",
        "        '-vf', 'scale=trunc(iw/2)*2:trunc(ih/2)*2',\n",
        "        '-c:v', 'libx264',\n",
        "        '-pix_fmt', 'yuv420p',\n",
        "        output_path\n",
        "    ]\n",
        "    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if result.returncode != 0:\n",
        "        raise RuntimeError(f\"La conversión con ffmpeg falló: {result.stderr}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "3AXMyK_jMLe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Procesamiento general de un archivo (imagen o video)\n",
        "\n",
        "def process_single_file(file_path, censor_method='blur', blur_ksize=35):\n",
        "    global known_embeddings\n",
        "\n",
        "    output_dir = \"processed_media\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    base_name = os.path.basename(file_path)\n",
        "    name, ext = os.path.splitext(base_name)\n",
        "\n",
        "    file_path_to_process = file_path\n",
        "    temp_mp4_path = None\n",
        "\n",
        "    if ext.lower() == '.webm':\n",
        "        temp_mp4_path = os.path.join(output_dir, f\"temp_{name}.mp4\")\n",
        "        file_path_to_process = convert_webm_to_mp4(file_path, temp_mp4_path)\n",
        "\n",
        "    # Procesamiento si es video\n",
        "    if is_video(file_path_to_process):\n",
        "        cap = cv2.VideoCapture(file_path_to_process)\n",
        "        if not cap.isOpened():\n",
        "            if temp_mp4_path and os.path.exists(temp_mp4_path):\n",
        "                os.remove(temp_mp4_path)\n",
        "            raise ValueError(f\"No se pudo abrir el video: {file_path_to_process}\")\n",
        "\n",
        "        # Configuración del archivo de salida\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "        output_path = os.path.join(output_dir, f\"censored_video_{name}.mp4\")\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "        if not out.isOpened():\n",
        "            cap.release()\n",
        "            if temp_mp4_path and os.path.exists(temp_mp4_path):\n",
        "                os.remove(temp_mp4_path)\n",
        "            raise ValueError(f\"No se pudo crear el archivo de salida de video: {output_path}. Asegúrate de que los códecs estén disponibles.\")\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = recognize_and_censor(frame, known_embeddings, censor_method, blur_ksize)\n",
        "            out.write(frame)\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Elimina archivo temporal si fue creado\n",
        "        if temp_mp4_path and os.path.exists(temp_mp4_path):\n",
        "            os.remove(temp_mp4_path)\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    # Procesamiento si es imagen\n",
        "    else:\n",
        "        image = cv2.imread(file_path_to_process)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"No se pudo abrir la imagen: {file_path_to_process}\")\n",
        "\n",
        "        # Aplica censura a la imagen\n",
        "        image = recognize_and_censor(image, known_embeddings, censor_method, blur_ksize)\n",
        "        output_path = os.path.join(output_dir, f\"censored_image_{name}.jpg\")\n",
        "        cv2.imwrite(output_path, image)\n",
        "        return output_path"
      ],
      "metadata": {
        "id": "d-X66v12MT4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Gradio\n",
        "\n",
        "if USE_GRADIO:\n",
        "\n",
        "    # Carga de rostros conocidos\n",
        "    def load_known_faces_gradio(face_files):\n",
        "        global known_embeddings, known_names\n",
        "        known_embeddings = []\n",
        "        known_names = []\n",
        "\n",
        "        if face_files:\n",
        "            for face_file in face_files:\n",
        "                img_path = face_file.name\n",
        "                img_pil = Image.open(img_path).convert(\"RGB\")\n",
        "                name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "                img_np_rgb = np.array(img_pil)\n",
        "                faces_in_known_img = app.get(img_np_rgb)\n",
        "\n",
        "                if len(faces_in_known_img) == 0:\n",
        "                    print(f\"Advertencia: No se detectó ningún rostro en '{name}'. Saltando este archivo.\")\n",
        "                    continue\n",
        "                elif len(faces_in_known_img) > 1:\n",
        "                    print(f\"Advertencia: Se detectaron múltiples rostros en '{name}'. Se usará el primer rostro detectado para el embedding.\")\n",
        "\n",
        "                face = faces_in_known_img[0]\n",
        "                box = face.bbox\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "                x1, y1 = max(0, x1), max(0, y1)\n",
        "                x2, y2 = min(img_np_rgb.shape[1], x2), min(img_np_rgb.shape[0], y2)\n",
        "\n",
        "                cropped_face_np = img_np_rgb[y1:y2, x1:x2]\n",
        "                if cropped_face_np.size == 0:\n",
        "                    print(f\"Advertencia: El recorte del rostro en '{name}' está vacío. Saltando.\")\n",
        "                    continue\n",
        "\n",
        "                cropped_face_pil = Image.fromarray(cropped_face_np)\n",
        "\n",
        "                emb = get_embedding(cropped_face_pil)\n",
        "\n",
        "                if emb is not None:\n",
        "                    known_embeddings.append(emb)\n",
        "                    known_names.append(name)\n",
        "                else:\n",
        "                    print(f\"Advertencia: No se pudo obtener la incrustación para el rostro en '{name}'. Saltando.\")\n",
        "            return f\"{len(known_embeddings)} rostros conocidos cargados.\"\n",
        "        return \"No se cargaron rostros conocidos.\"\n",
        "\n",
        "    # Función que procesa múltiples archivos, los guarda y genera un ZIP\n",
        "    def process_and_zip_and_display(files_to_process, censor_method, blur_ksize):\n",
        "        output_dir = \"processed_media\"\n",
        "        if os.path.exists(output_dir):\n",
        "            shutil.rmtree(output_dir)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        output_dir_for_zip = \"processed_media_for_zip_temp\"\n",
        "        if os.path.exists(output_dir_for_zip):\n",
        "            shutil.rmtree(output_dir_for_zip)\n",
        "        os.makedirs(output_dir_for_zip, exist_ok=True)\n",
        "\n",
        "        if not files_to_process:\n",
        "            return (\n",
        "                \"Por favor, sube al menos una imagen o video para procesar.\",\n",
        "                None,\n",
        "                gr.update(visible=False)\n",
        "            )\n",
        "\n",
        "        processed_images_paths = []\n",
        "        status_messages = []\n",
        "\n",
        "        for uploaded_file in files_to_process:\n",
        "            try:\n",
        "                output_path = process_single_file(uploaded_file.name, censor_method, blur_ksize)\n",
        "                if not is_video(output_path):\n",
        "                    processed_images_paths.append(output_path)\n",
        "                status_messages.append(f\"{os.path.basename(uploaded_file.name)} procesado correctamente. Salida: {output_path}\")\n",
        "            except Exception as e:\n",
        "                status_messages.append(f\"Error procesando {os.path.basename(uploaded_file.name)}: {str(e)}\")\n",
        "\n",
        "        # Genera ZIP con todos los archivos procesados si hay alguno\n",
        "        zip_path = None\n",
        "        if os.listdir(output_dir):\n",
        "            zip_base_name = os.path.join(output_dir_for_zip, \"processed_media_output\")\n",
        "            shutil.make_archive(zip_base_name, 'zip', output_dir)\n",
        "            zip_path = zip_base_name + \".zip\"\n",
        "\n",
        "        return (\n",
        "            \"\\n\".join(status_messages),\n",
        "            processed_images_paths if processed_images_paths else None,\n",
        "            gr.File(label=\"Descargar todos los medios procesados (ZIP)\", value=zip_path, visible=True if zip_path else False)\n",
        "        )\n",
        "\n",
        "    # Interfaz Gradio\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# Censurador de Rostros con Detección Avanzada\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### 1. Cargar Rostros Conocidos\")\n",
        "        gr.Markdown(\"Sube imágenes de los rostros que no quieres que sean censurados.\")\n",
        "        known_faces_input = gr.File(\n",
        "            label=\"Archivos de Rostros Conocidos (JPEG/PNG)\",\n",
        "            file_count=\"multiple\",\n",
        "            type=\"filepath\",\n",
        "            file_types=[\"image\"]\n",
        "        )\n",
        "        load_button = gr.Button(\"Cargar Rostros Conocidos\")\n",
        "        load_output = gr.Textbox(label=\"Estado de Carga de Rostros\")\n",
        "        load_button.click(load_known_faces_gradio, inputs=known_faces_input, outputs=load_output)\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### 2. Procesar Medios\")\n",
        "        gr.Markdown(\"Sube las imágenes o videos que deseas procesar para censurar rostros.\")\n",
        "        file_input = gr.File(\n",
        "            label=\"Imágenes o Videos (JPEG/PNG/MP4/MOV/WEBM)\",\n",
        "            file_count=\"multiple\",\n",
        "            type=\"filepath\"\n",
        "        )\n",
        "        censor_method_radio = gr.Radio(\n",
        "            [\"blur\"],\n",
        "            label=\"Método de Censura\",\n",
        "            value=\"blur\"\n",
        "        )\n",
        "        blur_ksize_slider = gr.Slider(\n",
        "            minimum=5,\n",
        "            maximum=101,\n",
        "            step=2,\n",
        "            value=35,\n",
        "            label=\"Tamaño de Censura (Cuanto mayor, más intenso el efecto)\"\n",
        "        )\n",
        "        process_button = gr.Button(\"Procesar Medios\")\n",
        "\n",
        "        processing_status = gr.Textbox(label=\"Estado de Procesamiento\", lines=5)\n",
        "        output_image_gallery = gr.Gallery(\n",
        "            label=\"Imágenes Procesadas (Solo imágenes mostradas aquí, videos en el ZIP)\",\n",
        "            columns=4,\n",
        "            height=\"auto\",\n",
        "            object_fit=\"contain\",\n",
        "            visible=True\n",
        "        )\n",
        "        output_zip_file = gr.File(\n",
        "            label=\"Descargar todos los medios procesados (ZIP)\",\n",
        "            visible=False\n",
        "        )\n",
        "\n",
        "        process_button.click(\n",
        "            process_and_zip_and_display,\n",
        "            inputs=[file_input, censor_method_radio, blur_ksize_slider],\n",
        "            outputs=[processing_status, output_image_gallery, output_zip_file]\n",
        "        )\n",
        "\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "9mmtLcd2-jyf",
        "outputId": "248dedbc-c8ab-4128-dc77-04e047351746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://1db54dc7b3a8aa9249.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1db54dc7b3a8aa9249.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advertencia: No se detectó ningún rostro en 'authorized'. Saltando este archivo.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1db54dc7b3a8aa9249.gradio.live\n"
          ]
        }
      ]
    }
  ]
}